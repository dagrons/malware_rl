import os
import random
import sys

import gym
import numpy as np
from gym import wrappers
from stable_baselines3 import A2C, PPO
from stable_baselines3.common.env_util import make_vec_env
from malware_rl.envs.ember_gym import ACTION_LOOKUP
import lief

import malware_rl

random.seed(0)
module_path = os.path.split(os.path.abspath(sys.modules[__name__].__file__))[0]
outdir = os.path.join(module_path, "data/logs/ppo-agent-results")

# Setting up environment
env = gym.make("malconv-train-v0")
env = wrappers.Monitor(env, directory=outdir, force=True)
env.seed(0)

# Setting up training parameters and holding variables
episode_count = 2000
done = False
reward = 0
evasions = 0
evasion_history = {}

# Train the agent
agent = PPO("MlpPolicy", env, n_steps=2048, verbose=1)
agent.learn(total_timesteps=2500)


# Test the agentls
for i in range(episode_count):
    env = gym.make("malconv-train-v0")
    env = wrappers.Monitor(env, directory=outdir, force=True)
    ob = env.reset()
    sha256 = env.env.sha256
    while True:
        action, _states = agent.predict(ob, reward, done)
        try:
            ob, reward, done, ep_history = env.step(action)
            print("choose action: ", ACTION_LOOKUP[action])
        except (RuntimeError, UnicodeDecodeError, TypeError, lief.bad_format, lief.read_out_of_bound) as e:
            print(e)
            break
        if done and reward >= 10.0:
            evasions += 1
            evasion_history[sha256] = ep_history
            break

        elif done:
            break

# Output metrics/evaluation stuff
evasion_rate = (evasions / episode_count) * 100
mean_action_count = np.mean(env.get_episode_lengths())
print(f"{evasion_rate}% samples evaded model.")
print(f"Average of {mean_action_count} moves to evade model.")
