import argparse
import os
import random
import sys
from datetime import datetime

import gym
import numpy as np
from stable_baselines3.a2c import A2C
from stable_baselines3.common.callbacks import CallbackList, CheckpointCallback
from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm
from stable_baselines3.ppo import PPO

import malware_rl


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--exp-id', type=str,
                        default=str(datetime.now()) + str(random.randint(0, 1000)))
    parser.add_argument('--train-env', type=str,
                        required=True, default="ember-train-v0")
    parser.add_argument('--test-env', type=str,
                        required=True, default="ember-test-v0")
    parser.add_argument('--agent', type=str, default="PPO")
    args = parser.parse_args()
    exp_id = "{}-{}-{}-{}".format(args.agent,
                                  args.train_env, args.test_env, args.exp_id)
    train_args = {
        "total_timesteps": 2048*16,
        "episode_count": 1000,
        "outdir": os.path.join("data/logs", exp_id),
        "save_freq": 4096,
        "save_path": "models/",
    }
    agent_args = {
        "policy": "MlpPolicy",
        "n_steps": 256,  # 16 x 256
        "verbose": 1,
        "tensorboard_log": os.path.join("logs/", exp_id)
    }
    registerd_agent_cls = {
        "PPO": PPO,
        "A2C": A2C,
    }
    train(exp_id=exp_id, agent_cls=registerd_agent_cls[args.agent],
          agent_args=agent_args, train_env_name=args.train_env,
          test_env_name=args.test_env, train_args=train_args)


def train(exp_id, agent_cls, agent_args, train_env_name, test_env_name, train_args, train_env_args={}, test_env_args={}):
    # initialize train env and test env
    train_env: gym.Env = gym.make(train_env_name, **train_env_args)
    train_env = gym.wrappers.Monitor(
        train_env, directory=os.path.join(train_args['outdir'], "train"), force=True)
    test_env: gym.Env = gym.make(test_env_name, **test_env_args)
    test_env = gym.wrappers.Monitor(
        train_env, directory=os.path.join(train_args['outdir'], "test"), force=True)

    # initialize agent
    agent_args["env"] = train_env
    agent: OnPolicyAlgorithm = agent_cls(**agent_args)  # type annotation

    # train the agent
    agent.learn(
        total_timesteps=train_args["total_timesteps"],
        eval_env=test_env,
        tb_log_name=exp_id,
        callback=CheckpointCallback(  # we can define custom callbacks
            save_freq=train_args["save_freq"], save_path=train_args["save_path"], name_prefix=exp_id),
    )

    # test the agent
    evasions = 0
    evasion_history = {}
    for i in range(train_args["episode_count"]):
        ob = test_env.reset()
        sha256 = test_env.sha256
        done = False
        while True:
            action, _ = agent.act(ob)
            ob, reward, done, ep_history = test_env.step(action)
            if done and reward >= 10.0:
                evasions += 1
                evasion_history[sha256] = ep_history
            elif done:
                break
    evasion_rate = (evasions / train_args["episode_count"]) * 100
    mean_action_count = np.mean(test_env.get_episode_lengths())
    print(f"{evasion_rate}% samples evaded model.")
    print(f"Average of {mean_action_count} moves to evade model.")


if __name__ == "__main__":
    main()
